var documenterSearchIndex = {"docs":
[{"location":"man/causal/#Causal-model","page":"Causal model","title":"Causal model","text":"","category":"section"},{"location":"man/causal/","page":"Causal model","title":"Causal model","text":"Knowing how individual skills change over time is essential in many areas.  Since skills are hidden variables, the best we can do is estimating them based on its direct observable consequences: the outcome of problem-solving and competitions.  Considering only the frequency of positive results as an indicator of the individuals' ability could lead to wrong approximations, mainly because the outcome also depends on the difficulty of the challenge.  For this reason, all widely used skill estimators are based on pairwise comparisons. All currently used skill estimators share some variant of the following causal model:","category":"page"},{"location":"man/causal/","page":"Causal model","title":"Causal model","text":"(Image: )","category":"page"},{"location":"man/causal/","page":"Causal model","title":"Causal model","text":"This is a generative model in which skills (s) cause the observable results (r) mediated by the difference of hidden performances, d =p_i - p_j. Even if the skills are constant at a given point in time, the performances are random variables around their unknown true skill, p sim mathcalN(sbeta^2). The model assumes that the agent with the highest performance wins, r = (d  0). Observable variables are painted gray, hidden are transparent, and constants are shown as dots. ","category":"page"},{"location":"man/causal/#The-scale-of-estimates","page":"Causal model","title":"The scale of estimates","text":"","category":"section"},{"location":"man/causal/","page":"Causal model","title":"Causal model","text":"The standard deviation of performances beta, is the same for all the agents, acts as the scale of the estimates. A real skill difference of one beta between two agents is equivalent to 76% probability of winning in favor of the stronger agent. For this reason we choose the default value to be 1.","category":"page"},{"location":"man/causal/","page":"Causal model","title":"Causal model","text":"BETA","category":"page"},{"location":"man/causal/#TrueSkillThroughTime.BETA","page":"Causal model","title":"TrueSkillThroughTime.BETA","text":"The default standar deviation of the performances is\n\nglobal const BETA = 1.0\n\nThis parameter acts as the scale of the estimates. A real difference of one beta between two skills is equivalent to 76% probability of winning.\n\n\n\n\n\n","category":"constant"},{"location":"man/causal/#The-prior","page":"Causal model","title":"The prior","text":"","category":"section"},{"location":"man/causal/","page":"Causal model","title":"Causal model","text":"The causal model assumes that, at a given time, the skills are constant. However, we do not know this value. To represent our uncertainty we use a Gaussian distribution.","category":"page"},{"location":"man/causal/","page":"Causal model","title":"Causal model","text":"p(s) = mathcalN(mu sigma^2)","category":"page"},{"location":"man/causal/","page":"Causal model","title":"Causal model","text":"The initial mean (mu) can be freely chosen because it is the difference of skills that matters and not its absolute value.","category":"page"},{"location":"man/causal/","page":"Causal model","title":"Causal model","text":"MU","category":"page"},{"location":"man/causal/#TrueSkillThroughTime.MU","page":"Causal model","title":"TrueSkillThroughTime.MU","text":"The default mean of the priors is\n\nglobal const MU = 0.0\n\nused by the Gaussian class\n\n\n\n\n\n","category":"constant"},{"location":"man/causal/","page":"Causal model","title":"Causal model","text":"The prior's standard deviation (sigma) must be sufficiently large to include all possible skill hypotheses. For this reason we chose it to be 6 times larger than the standard deviation of the performance.","category":"page"},{"location":"man/causal/","page":"Causal model","title":"Causal model","text":"SIGMA","category":"page"},{"location":"man/causal/#TrueSkillThroughTime.SIGMA","page":"Causal model","title":"TrueSkillThroughTime.SIGMA","text":"The default standar deviation of the priors is\n\nglobal const SIGMA = (BETA * 6)\n\nused by the Gaussian class\n\n\n\n\n\n","category":"constant"},{"location":"man/causal/#The-dynamic-factor","page":"Causal model","title":"The dynamic factor","text":"","category":"section"},{"location":"man/causal/","page":"Causal model","title":"Causal model","text":"Since skills change over time, it is important to incorporate some uncertainty (gamma) after each time step.","category":"page"},{"location":"man/causal/","page":"Causal model","title":"Causal model","text":"p(s_t) = mathcalN(s_t  mu_t-1 sigma_t-1^2 + gamma^2 )","category":"page"},{"location":"man/causal/","page":"Causal model","title":"Causal model","text":"where mu_t-1 and sigma_t-1 are the mean and standard deviation of the skill estimate at the previous time. As its optimal value is generally relatively low, we chose the default value to be 3% of the standard deviation of the performances.","category":"page"},{"location":"man/causal/","page":"Causal model","title":"Causal model","text":"GAMMA","category":"page"},{"location":"man/causal/#TrueSkillThroughTime.GAMMA","page":"Causal model","title":"TrueSkillThroughTime.GAMMA","text":"The default amount of uncertainty (standar deviation) added to the estimates as time progresses\n\nglobal const GAMMA = (BETA * 0.03)\n\n\n\n\n\n","category":"constant"},{"location":"man/causal/#The-draw-probability","page":"Causal model","title":"The draw probability","text":"","category":"section"},{"location":"man/causal/","page":"Causal model","title":"Causal model","text":"A rule of thumb states that the probability of a draw must be initialized with the observed frequency of draws. If in doubt, it is a candidate parameter to be optimized or integrated by the sum rule. It is used to compute the prior probability of the observed result, so its value may affect an eventual model selection task. The default value is 0.","category":"page"},{"location":"man/causal/","page":"Causal model","title":"Causal model","text":"P_DRAW","category":"page"},{"location":"man/causal/#TrueSkillThroughTime.P_DRAW","page":"Causal model","title":"TrueSkillThroughTime.P_DRAW","text":"The default probability of a draw is \n\nglobal const P_DRAW = 0.0\n\n\n\n\n\n","category":"constant"},{"location":"man/gaussian/#gaussian","page":"The Gaussian class","title":"The Gaussian class","text":"","category":"section"},{"location":"man/gaussian/","page":"The Gaussian class","title":"The Gaussian class","text":"using TrueSkillThroughTime\nttt = TrueSkillThroughTime","category":"page"},{"location":"man/gaussian/","page":"The Gaussian class","title":"The Gaussian class","text":"The Gaussian class does most of the computation of the packages.","category":"page"},{"location":"man/gaussian/","page":"The Gaussian class","title":"The Gaussian class","text":"Pages = [\"gaussian.md\"]","category":"page"},{"location":"man/gaussian/","page":"The Gaussian class","title":"The Gaussian class","text":"Gaussian","category":"page"},{"location":"man/gaussian/#TrueSkillThroughTime.Gaussian","page":"The Gaussian class","title":"TrueSkillThroughTime.Gaussian","text":"The Gaussian class is used to define the prior beliefs of the agents' skills (and for internal computations).\n\nWe can create objects by passing the parameters in order or by mentioning the names. \n\nGaussian(mu::Float64=MU, sigma::Float64=SIGMA)\nGaussian(;mu::Float64=MU, sigma::Float64=SIGMA)\n\nmu is the mean of the Gaussian distribution\nsigma is the standar deviation of the Gaussian distribution\n\n\n\n\n\n","category":"type"},{"location":"man/gaussian/","page":"The Gaussian class","title":"The Gaussian class","text":"The default value of MU and SIGMA are","category":"page"},{"location":"man/gaussian/","page":"The Gaussian class","title":"The Gaussian class","text":"N06 = ttt.Gaussian()","category":"page"},{"location":"man/gaussian/","page":"The Gaussian class","title":"The Gaussian class","text":"Others ways to create Gaussian objects","category":"page"},{"location":"man/gaussian/","page":"The Gaussian class","title":"The Gaussian class","text":"N01 = ttt.Gaussian(sigma = 1.0)\nN12 = ttt.Gaussian(1.0, 2.0)\nNinf = ttt.Gaussian(1.0,Inf)\nprintln(\"mu: \", N01.mu, \", sigma: \", N01.sigma)","category":"page"},{"location":"man/gaussian/","page":"The Gaussian class","title":"The Gaussian class","text":"The class overwrites the addition +, subtraction -, product *, and division / to compute the marginal distributions used in the TrueSkill Through Time model.","category":"page"},{"location":"man/gaussian/#Product-*","page":"The Gaussian class","title":"Product *","text":"","category":"section"},{"location":"man/gaussian/","page":"The Gaussian class","title":"The Gaussian class","text":"k mathcalN(xmusigma^2) = mathcalN(xkmu(ksigma)^2)\nmathcalN(xmu_1sigma_1^2)mathcalN(xmu_2sigma_2^2) propto mathcalN(xmu_*sigma_*^2)","category":"page"},{"location":"man/gaussian/","page":"The Gaussian class","title":"The Gaussian class","text":"with fracmu_*sigma_*^2 = fracmu_1sigma_1^2 + fracmu_2sigma_2^2 and sigma_*^2 = (frac1sigma_1^2 + frac1sigma_2^2)^-1.","category":"page"},{"location":"man/gaussian/","page":"The Gaussian class","title":"The Gaussian class","text":"*","category":"page"},{"location":"man/gaussian/#Base.:*","page":"The Gaussian class","title":"Base.:*","text":"*(N::Gaussian, M::Gaussian)\n\n\n\n\n\n+(k::Float64, M::Gaussian)\n\n\n\n\n\n","category":"function"},{"location":"man/gaussian/","page":"The Gaussian class","title":"The Gaussian class","text":"N06 * N12\nN12 * 5.0\nN12 * Ninf","category":"page"},{"location":"man/gaussian/#Division-/","page":"The Gaussian class","title":"Division /","text":"","category":"section"},{"location":"man/gaussian/","page":"The Gaussian class","title":"The Gaussian class","text":"mathcalN(xmu_1sigma_1^2)mathcalN(xmu_2sigma_2^2) propto mathcalN(xmu_divsigma_div^2)","category":"page"},{"location":"man/gaussian/","page":"The Gaussian class","title":"The Gaussian class","text":"with fracmu_divsigma_div^2 = fracmu_1sigma_1^2 - fracmu_2sigma_2^2 and sigma_div^2 = (frac1sigma_1^2 - frac1sigma_2^2)^-1.","category":"page"},{"location":"man/gaussian/","page":"The Gaussian class","title":"The Gaussian class","text":"/","category":"page"},{"location":"man/gaussian/#Base.:/","page":"The Gaussian class","title":"Base.:/","text":"/(N::Gaussian, M::Gaussian)\n\n\n\n\n\n","category":"function"},{"location":"man/gaussian/","page":"The Gaussian class","title":"The Gaussian class","text":"N12 / N06\nN12 / Ninf","category":"page"},{"location":"man/gaussian/#Addition","page":"The Gaussian class","title":"Addition +","text":"","category":"section"},{"location":"man/gaussian/","page":"The Gaussian class","title":"The Gaussian class","text":"iint delta(t=x + y) mathcalN(xmu_1 sigma_1^2)mathcalN(ymu_2 sigma_2^2) dxdy =  mathcalN(tmu_1+mu_2sigma_1^2 + sigma_2^2)","category":"page"},{"location":"man/gaussian/","page":"The Gaussian class","title":"The Gaussian class","text":"+","category":"page"},{"location":"man/gaussian/#Base.:+","page":"The Gaussian class","title":"Base.:+","text":"+(N::Gaussian, M::Gaussian)\n\n\n\n\n\n","category":"function"},{"location":"man/gaussian/","page":"The Gaussian class","title":"The Gaussian class","text":"N06 + N12","category":"page"},{"location":"man/gaussian/#Substraction","page":"The Gaussian class","title":"Substraction -","text":"","category":"section"},{"location":"man/gaussian/","page":"The Gaussian class","title":"The Gaussian class","text":"iint delta(t=x - y) mathcalN(xmu_1 sigma_1^2)mathcalN(ymu_2 sigma_2^2) dxdy =  mathcalN(tmu_1-mu_2sigma_1^2 + sigma_2^2) ","category":"page"},{"location":"man/gaussian/","page":"The Gaussian class","title":"The Gaussian class","text":"-","category":"page"},{"location":"man/gaussian/#Base.:-","page":"The Gaussian class","title":"Base.:-","text":"-(N::Gaussian, M::Gaussian)\n\n\n\n\n\n","category":"function"},{"location":"man/gaussian/","page":"The Gaussian class","title":"The Gaussian class","text":"N06 - N12","category":"page"},{"location":"man/gaussian/#Others-methods","page":"The Gaussian class","title":"Others methods","text":"","category":"section"},{"location":"man/gaussian/#isapprox","page":"The Gaussian class","title":"isapprox","text":"","category":"section"},{"location":"man/gaussian/","page":"The Gaussian class","title":"The Gaussian class","text":"isapprox","category":"page"},{"location":"man/gaussian/#Base.isapprox","page":"The Gaussian class","title":"Base.isapprox","text":"isapprox(N::Gaussian, M::Gaussian, atol::Real=0)\n\n\n\n\n\n","category":"function"},{"location":"man/gaussian/","page":"The Gaussian class","title":"The Gaussian class","text":"N06-N12 == ttt.Gaussian(mu=-1.0, sigma=6.324555)\nttt.isapprox(N06-N12, ttt.Gaussian(mu=-1.0, sigma=6.324555), 1e-6)","category":"page"},{"location":"man/gaussian/#forget","page":"The Gaussian class","title":"forget","text":"","category":"section"},{"location":"man/gaussian/","page":"The Gaussian class","title":"The Gaussian class","text":"forget","category":"page"},{"location":"man/gaussian/#TrueSkillThroughTime.forget","page":"The Gaussian class","title":"TrueSkillThroughTime.forget","text":"forget(N::Gaussian, gamma::Float64, t::Int64=1)\n\n\n\n\n\n","category":"function"},{"location":"man/history/#The-History-class","page":"The History class","title":"The History class","text":"","category":"section"},{"location":"man/history/","page":"The History class","title":"The History class","text":"using TrueSkillThroughTime\nttt = TrueSkillThroughTime","category":"page"},{"location":"man/history/","page":"The History class","title":"The History class","text":"We use the History class to compute the learning curves and predictions of a sequence of events.","category":"page"},{"location":"man/history/","page":"The History class","title":"The History class","text":"Pages = [\"history.md\"]","category":"page"},{"location":"man/history/","page":"The History class","title":"The History class","text":"History","category":"page"},{"location":"man/history/#TrueSkillThroughTime.History","page":"The History class","title":"TrueSkillThroughTime.History","text":"The History class\n\nHistory(composition::Vector{Vector{Vector{String}}},\nresults::Vector{Vector{Float64}}=Vector{Vector{Float64}}(),\ntimes::Vector{Int64}=Int64[], priors::Dict{String,Player}=Dict{String,Player}()\n; mu::Float64=MU, sigma::Float64=SIGMA, beta::Float64=BETA,\ngamma::Float64=GAMMA, p_draw::Float64=P_DRAW, online::Bool=false,\nweights::Vector{Vector{Vector{Float64}}}=Vector{Vector{Vector{Float64}}}())\n\nProperties:\n\nsize::Int64\nbatches::Vector{Batch}\nagents::Dict{String,Agent}\ntime::Bool\nmu::Float64\nsigma::Float64\nbeta::Float64\ngamma::Float64\np_draw::Float64\nonline::Bool\n\n\n\n\n\n","category":"type"},{"location":"man/history/","page":"The History class","title":"The History class","text":"Let us return to the example seen on the first page of this manual. We define the composition of each game using the names of the agents (i.e. their identifiers). In the following example, all agents (\"a\", \"b\", \"c\") win one game and lose the other.  The results will be implicitly defined by the order in which the game compositions are initialized: the teams appearing firstly in the list defeat those appearing later.  By initializing gamma = 0.0 we specify that skills do not change over time.","category":"page"},{"location":"man/history/","page":"The History class","title":"The History class","text":"c1 = [[\"a\"],[\"b\"]]\nc2 = [[\"b\"],[\"c\"]]\nc3 = [[\"c\"],[\"a\"]]\ncomposition = [c1, c2, c3]\nh = ttt.History(composition, gamma=0.0)","category":"page"},{"location":"man/history/","page":"The History class","title":"The History class","text":"After initialization, the History class immediately instantiates a new player for each name and activates the computation of the TrueSkill estimates (not yet TrueSkill Through Time).","category":"page"},{"location":"man/history/#Learning-curves","page":"The History class","title":"Learning curves","text":"","category":"section"},{"location":"man/history/","page":"The History class","title":"The History class","text":"To access estimates we can call the method learning\\_curves(), which returns a dictionary indexed by the names of the agents.","category":"page"},{"location":"man/history/","page":"The History class","title":"The History class","text":"learning_curves","category":"page"},{"location":"man/history/#TrueSkillThroughTime.learning_curves","page":"The History class","title":"TrueSkillThroughTime.learning_curves","text":"learning_curves(h::History)\n\n\n\n\n\n","category":"function"},{"location":"man/history/","page":"The History class","title":"The History class","text":"ttt.learning_curves(h)[\"a\"]","category":"page"},{"location":"man/history/","page":"The History class","title":"The History class","text":"ttt.learning_curves(h)[\"b\"]","category":"page"},{"location":"man/history/","page":"The History class","title":"The History class","text":"Individual learning curves are lists of tuples: each tuple has the time of the estimate as the first component and the estimate itself as the second one. Although in this example no player is stronger than the others, the TrueSkill estimates present strong variations between players.","category":"page"},{"location":"man/history/#Convergence","page":"The History class","title":"Convergence","text":"","category":"section"},{"location":"man/history/","page":"The History class","title":"The History class","text":"TrueSkill Through Time solves TrueSkill's inability to obtain correct estimates by allowing the information to propagate throughout the system. To compute them, we call the method convergence() of the History class.","category":"page"},{"location":"man/history/","page":"The History class","title":"The History class","text":"convergence(h::History)","category":"page"},{"location":"man/history/#TrueSkillThroughTime.convergence-Tuple{History}","page":"The History class","title":"TrueSkillThroughTime.convergence","text":"convergence(h::History; epsilon::Float64=EPSILON,\niterations::Int64=ITERATIONS; epsilon::Float64=EPSILON, iterations::Int64=ITERATIONS, verbose = true)\n\n\n\n\n\n","category":"method"},{"location":"man/history/","page":"The History class","title":"The History class","text":"TrueSkill Through Time not only returns correct estimates (same for all players), they also have less uncertainty.","category":"page"},{"location":"man/history/","page":"The History class","title":"The History class","text":"ttt.convergence(h)\nttt.learning_curves(h)[\"a\"]","category":"page"},{"location":"man/history/","page":"The History class","title":"The History class","text":"ttt.learning_curves(h)[\"b\"]","category":"page"},{"location":"man/history/#Model-evidence","page":"The History class","title":"Model evidence","text":"","category":"section"},{"location":"man/history/","page":"The History class","title":"The History class","text":"We would like to have a procedure to decide whether TrueSkill Through Time is better than others models and the optimal values of the parameters sigma and gamma. In the same way that we use probability theory to evaluate the hypotheses of a model given the data, we can also evaluate different models given the data.","category":"page"},{"location":"man/history/","page":"The History class","title":"The History class","text":"P(textModeltextData) propto P(textDatatextModel)P(textModel)","category":"page"},{"location":"man/history/","page":"The History class","title":"The History class","text":"where P(textModel) is the prior of the models, which we define, and P(textDatatextModel) is the prediction made by the model. In the special case where we have no prior preference over any model, we need only compare the predictions made by the models.","category":"page"},{"location":"man/history/","page":"The History class","title":"The History class","text":"P(textModeltextData) propto P(textDatatextModel)","category":"page"},{"location":"man/history/","page":"The History class","title":"The History class","text":"In other words, we prefer the model with the best prediction.","category":"page"},{"location":"man/history/","page":"The History class","title":"The History class","text":"P(textDatatextModel) = P(d_1textM)P(d_2d_1textM) dots P(d_nd_n-1 dots d_1 textM)","category":"page"},{"location":"man/history/","page":"The History class","title":"The History class","text":"where D represents the data set, M the model, and d_i the individual data points. This measure can be obtained by the evidence method.","category":"page"},{"location":"man/history/","page":"The History class","title":"The History class","text":"log_evidence","category":"page"},{"location":"man/history/#TrueSkillThroughTime.log_evidence","page":"The History class","title":"TrueSkillThroughTime.log_evidence","text":"log_evidence(h::History; agents::Vector{String} = Vector{String}(), forward::Bool = false)\n\n\n\n\n\n","category":"function"},{"location":"man/history/","page":"The History class","title":"The History class","text":"Let us develop a complex synthetic example in which this measure is useful for choosing the optimal dynamic uncertainty.","category":"page"},{"location":"man/history/#Optimizing-the-dynamic-factor","page":"The History class","title":"Optimizing the dynamic factor","text":"","category":"section"},{"location":"man/history/","page":"The History class","title":"The History class","text":"We now analyze a scenario in which a new player joins a large community of already known players. In this example, we focus on the estimation of an evolving skill. For this purpose, we establish the skill of the target player to change over time following a logistic function. The community is generated by ensuring that each opponent has a skill similar to that of the target player throughout their evolution.  In the following code, we generate the target player's learning curve and 1000 random opponents. ","category":"page"},{"location":"man/history/","page":"The History class","title":"The History class","text":"using Random; Random.seed!(999); N = 1000\nfunction skill(experience, middle, maximum, slope)\n    return maximum/(1+exp(slope*(-experience+middle)))\nend\ntarget = skill.(1:N, 500, 2, 0.0075)\nopponents = Random.randn.(1000)*0.5 .+ target\nprintln(\"t1 = \", target[1], \", tn = \", target[end])","category":"page"},{"location":"man/history/","page":"The History class","title":"The History class","text":"The list target has the agent's skills at each moment: the values start at zero and grow smoothly until the target player's skill reaches two. The list opponents includes the randomly generated opponents' skills following a Gaussian distribution centered on each target's skills and a standard deviation of 05.","category":"page"},{"location":"man/history/","page":"The History class","title":"The History class","text":"composition = [[[\"a\"], [string(i)]] for i in 1:N]\nresults = [r ? [1.,0.] : [0.,1.] for r in (Random.randn(N).+target.>Random.randn(N).+opponents)]\ntimes = [i for i in 1:N]\npriors = Dict{String,ttt.Player}()\nfor i in 1:N  priors[string(i)] = ttt.Player(ttt.Gaussian(opponents[i], 0.2))  end\n\nh = ttt.History(composition, results, times, priors, gamma=0.018)\nttt.convergence(h, iterations = 16)\nmu = [tp[2].mu for tp in ttt.learning_curves(h)[\"a\"]]\nprintln()","category":"page"},{"location":"man/history/","page":"The History class","title":"The History class","text":"In this code, we define four variables to instantiate the class History to compute the target's learning curve. The variable composition contains 1000 games between the target player and different opponents. The list results is generated randomly by sampling the agents' performance following Gaussian distributions centered on their skills. The winner is the player with the highest performance. The variable time is a list of integer values ranging from 0 to 999 representing the time batch in which each game is located: the class History uses the temporal distance between events to determine the amount of dynamic uncertainty (gamma^2) to be added between games. The variable priors is a dictionary used to customize player attributes: we assign low uncertainty to the opponents' priors as we know their skills beforehand.","category":"page"},{"location":"man/history/","page":"The History class","title":"The History class","text":"The class History receives these four parameters and initializes the target player using the default values and a dynamic uncertainty gamma=0.018. Using the method convergence(), we obtain the TrueSkill Through Time estimates and the target's learning curve. The following figure shows the evolution of the true (solid line) and estimated (dotted line) target player's learning curves.","category":"page"},{"location":"man/history/","page":"The History class","title":"The History class","text":"(Image: )","category":"page"},{"location":"man/history/","page":"The History class","title":"The History class","text":"The estimated learning curves remain close to the actual skill during the whole evolution.","category":"page"},{"location":"man/history/","page":"The History class","title":"The History class","text":"le = ttt.log_evidence(h)","category":"page"},{"location":"man/history/","page":"The History class","title":"The History class","text":"The geometric mean of the evidence is","category":"page"},{"location":"man/history/","page":"The History class","title":"The History class","text":"exp(le/h.size)","category":"page"},{"location":"man/history/","page":"The History class","title":"The History class","text":"To optimize, repeat this procedure with different values of gamma until minimize the log_evidence (or maximize the geommetric mean). ","category":"page"},{"location":"man/player/#The-Player-class","page":"The Player class","title":"The Player class","text":"","category":"section"},{"location":"man/player/","page":"The Player class","title":"The Player class","text":"using TrueSkillThroughTime\nttt = TrueSkillThroughTime","category":"page"},{"location":"man/player/","page":"The Player class","title":"The Player class","text":"Pages = [\"player.md\"]","category":"page"},{"location":"man/player/","page":"The Player class","title":"The Player class","text":"The features of the agents are defined within class Player: the prior Gaussian distribution characterized by the mean (mu) and the standard deviation (sigma), the standard deviation of the performance (beta), and the dynamic uncertainty of the skill (gamma). ","category":"page"},{"location":"man/player/","page":"The Player class","title":"The Player class","text":"Player","category":"page"},{"location":"man/player/#TrueSkillThroughTime.Player","page":"The Player class","title":"TrueSkillThroughTime.Player","text":"The Player class is used to define the features of the agents. We can create objects by indicating the parameters in order or by mentioning their names. \n\nPlayer(prior::Gaussian=Gaussian(MU,SIGMA), beta::Float64=BETA, gamma::Float64=GAMMA)\nPlayer(;prior::Gaussian=Gaussian(MU,SIGMA), beta::Float64=BETA, gamma::Float64=GAMMA)\n\nprior is the prior belief distribution of skill hypotheses\nbeta is the standar deviation of the agent performance\ngamma is the uncertainty (standar deviation) added to the estimates as time progresses\n\n\n\n\n\n","category":"type"},{"location":"man/player/","page":"The Player class","title":"The Player class","text":"The default value of MU, SIGMA, BETA and GAMMA are ","category":"page"},{"location":"man/player/","page":"The Player class","title":"The Player class","text":"a1 = ttt.Player()","category":"page"},{"location":"man/player/","page":"The Player class","title":"The Player class","text":"a2 = ttt.Player(ttt.Gaussian(0.0, 1.0))","category":"page"},{"location":"man/player/","page":"The Player class","title":"The Player class","text":"We can also create special players who have non-random performances (beta=0.0), and whose skills do not change over time (gamma=0.0).","category":"page"},{"location":"man/player/","page":"The Player class","title":"The Player class","text":"a3 = ttt.Player(beta=0.0, gamma=0.0)\na3.beta\na3.gamma","category":"page"},{"location":"man/player/#Performance","page":"The Player class","title":"Performance","text":"","category":"section"},{"location":"man/player/","page":"The Player class","title":"The Player class","text":"The performances p are random variables around their unknown true skill s,","category":"page"},{"location":"man/player/","page":"The Player class","title":"The Player class","text":"p sim mathcalN(sbeta^2)","category":"page"},{"location":"man/player/","page":"The Player class","title":"The Player class","text":"performance(R::Player)","category":"page"},{"location":"man/player/#TrueSkillThroughTime.performance-Tuple{Player}","page":"The Player class","title":"TrueSkillThroughTime.performance","text":"performance(R::Player)\n\n\n\n\n\n","category":"method"},{"location":"man/player/","page":"The Player class","title":"The Player class","text":"ttt.performance(a2)\nttt.performance(a3)","category":"page"},{"location":"man/examples/#real_example","page":"Real examples","title":"Real examples","text":"","category":"section"},{"location":"man/examples/","page":"Real examples","title":"Real examples","text":"using TrueSkillThroughTime\nttt = TrueSkillThroughTime","category":"page"},{"location":"man/examples/","page":"Real examples","title":"Real examples","text":"Pages = [\"examples.github/workflows/TagBot.yml.md\"]","category":"page"},{"location":"man/examples/#The-History-of-the-Association-of-Tennis-Professionals","page":"Real examples","title":"The History of the Association of Tennis Professionals","text":"","category":"section"},{"location":"man/examples/","page":"Real examples","title":"Real examples","text":"In this last example, we analyze the complete history of the Association of Tennis Professionals (ATP) registered matches.  The database has 447000 games starting in 1915 until 2020 with more than 19000 participating players and is publicly available. The information stored in a single CSV file. Each game has an identifier (i.e. match_id) and its tournament's round number (i.e. round_number), where 0 represents the final game, 1 the semi-final, and so on. The file also contains players' identifiers and names. For example, column w2_id is the second player's identifier of the winning team, and l1_name is the first player's name of the losing team.  Finally, we have the tournament's name (tour_name), its identifier (tour_id), the tournament's starting date (time_start), and the type of surface (ground).","category":"page"},{"location":"man/examples/","page":"Real examples","title":"Real examples","text":"using CSV; using Dates\ndata = CSV.read(\"history.csv\")\n\ndates = Dates.value.(data[:,\"time_start\"] .- Date(\"1900-1-1\")) \nmatches = [ r.double == \"t\" ? [[r.w1_id,r.w2_id],[r.l1_id,r.l2_id]] : [[r.w1_id],[r.l1_id]] for r in eachrow(data) ]   \n\nh = ttt.History(composition = matches, times = dates, sigma = 1.6, gamma = 0.036)\nttt.convergence(h, epsilon = 0.01, iterations = 10)","category":"page"},{"location":"man/examples/","page":"Real examples","title":"Real examples","text":"In this code, we open the file atp.csv, create the variables dates and composition, and instantiate the class History. We define the event times as the days elapsed from a reference date to the tournament start date, assuming that the skill is the same within each tournament. When generating the list composition we discriminate whether the games are doubles or singles using the column double.  The results are determined by the composition's order, placing the winning team first. When initializing the class History we set the values of sigma and gamma based on an optimization procedure previously performed. Finally, we use the convergence() method to obtain TrueSkill Through Time estimates explicitly selecting the convergence criterion: when the change between iterations is less than 001 or when ten iterations are performed.","category":"page"},{"location":"man/examples/","page":"Real examples","title":"Real examples","text":"The following figure presents the estimated learning curves of some famous players in ATP's history, which we identified using different colors. The learning curves share a similar pattern: they begin with rapid growth, reach an unstable plateau, and end with a slow decline (we hidden the last portion of the players who have long final stages for visualization purposes).","category":"page"},{"location":"man/examples/","page":"Real examples","title":"Real examples","text":"(Image: )","category":"page"},{"location":"man/examples/","page":"Real examples","title":"Real examples","text":"The top bar indicates which player was at the top of the ATP's ranking (the bar has no color when player number 1 is not included among the 10 players identified with colors). ATP's ranking points are updated every Monday according to the prestige of the tournament and the stage reached.  There is a relative coincidence between the skill estimates and who is at any given moment at the top of the ATP rankings. The following Table shows the historical ranking of players in the top position of the ATP's ranking according to the number of weeks occupying the first position.","category":"page"},{"location":"man/examples/","page":"Real examples","title":"Real examples","text":"No Name Weeks at top\n1 Novak Djokovic 320\n2 Roger Federer 310\n3 Pete Sampras 286\n4 Ivan Lendl 270\n5 Jimmy Connors 268\n6 Rafael Nadal 209\n7 John McEnroe 170\n8 Bj\\\"orn Borg 109\n9 Andre Agassi 101\n10 Lleyton Hewitt 80\n11 Stefan Edberg 72\n12 Jim Courier 58\n13 Gustavo Kuerten 43\n14 Andy Murray 41\n15 Ilie N\\u{a}stase 40\n16 Mats Wilander 20","category":"page"},{"location":"man/examples/","page":"Real examples","title":"Real examples","text":"However, TrueSkill Through Time allows comparing the relative ability of players over time: the 10th player in the historical ATP's ranking, Hewitt, is a product of the window of opportunity that was opened in the year 2000; and the 4th most skilled player, Murray, is ranked 14th just above Nastase. Individual learning curves enable recognizing special periods of crisis and prolonged stability of the professional players, and even the effects of emotional slumps such as those suffered by Aggasi and Djokovic. It is worthwhile to note that the skill of tennis players did not increase abruptly over the years: contrary to what might have been expected, the players of the 1980s were more skilled than those of the 1990s, and reached a skill similar to what Federer, Nadal and Djokovic had in 2020, even though the latter reached higher values for a longer time.","category":"page"},{"location":"man/examples/#Multidimensional-skills","page":"Real examples","title":"Multidimensional skills","text":"","category":"section"},{"location":"man/examples/","page":"Real examples","title":"Real examples","text":"In the previous example, we summarize the players' skills in a single dimension. TrueSkill Through Time allows estimating multi-dimensional skills.  It is known that the ability of certain tennis players varies significantly depending on the surface. To quantify this phenomenon, we propose modeling each player as a team composed of a generic player, who is included in all the games, and another player representing their ability on a particular surface. For example, Nadal will be represented as a two-player team: Nadal_generic and Nadal_clay when playing on this kind of surface, and Nadal_generic and Nadal_grass when participating in the Wimbledon tournament.","category":"page"},{"location":"man/examples/","page":"Real examples","title":"Real examples","text":"players = Set(vcat((composition...)...))\npriors = Dict([(p, ttt.Player(ttt.Gaussian(0., 1.6), 1.0, 0.036) ) for p in players])\n\ncomposition_ground = [ r.double == \"t\" ? [[r.w1_id, r.w1_id*r.ground, r.w2_id, r.w2_id*r.ground],[r.l1_id, r.l1_id*r.ground, r.l2_id, r.l2_id*r.ground]] : [[r.w1_id, r.w1_id*r.ground],[r.l1_id, r.l1_id*r.ground]] for r in eachrow(data) ]   \n\nh_ground = ttt.History(composition = composition_ground, times = dates, sigma = 1.0, gamma = 0.01, beta = 0.0, priors = priors)\nttt.convergence(h_ground, epsilon = 0.01, iterations=10)","category":"page"},{"location":"man/examples/","page":"Real examples","title":"Real examples","text":"In this example, we keep the same prior as before for all the generic players, but in this code we define them using the variable priors. We create the teams depending on whether the game is double or single, adding the specific surface skills of each player as their teammate (we use the operator * to concatenate strings). As the specific surface skills are not defined in the variable prior, they are initialized using the default values defined in the class History. We also define beta as null for specific surface skills to avoid adding additional noise to the players' performance, keeping the scale of the estimates stable. We select a sigma that we consider sufficiently large and a dynamic factor gamma representing 1% of the prior uncertainty.","category":"page"},{"location":"man/examples/","page":"Real examples","title":"Real examples","text":"In the following Figures, we show the skill difference that Nadal and Djokovic have in each of the three types of ground.","category":"page"},{"location":"man/examples/","page":"Real examples","title":"Real examples","text":"(Image: )","category":"page"},{"location":"man/examples/","page":"Real examples","title":"Real examples","text":"Nadal has a notorious skill difference when playing on different surfaces.  The Nadal's skill difference between clay and grass grounds is greater than one beta, which means at least 76\\% difference in probability of winning compared to itself. On the contrary, Djokovic has very similar skills in the three types.","category":"page"},{"location":"man/examples/","page":"Real examples","title":"Real examples","text":"(Image: )","category":"page"},{"location":"man/examples/","page":"Real examples","title":"Real examples","text":"In the case of Nadal (id \"n409\"), it seems important to model the skill's multi-dimensionality, while in Djokovic's case (id \"d643\") it seems reasonable to summarize it in a single dimension. To assess whether the complexity added by modeling multi-dimensionality is appropriate in general terms, we can compare the joint prior prediction of the models, calling the method log_evidence() of the class History.","category":"page"},{"location":"man/examples/","page":"Real examples","title":"Real examples","text":"ttt.log_evidence(h_ground)","category":"page"},{"location":"man/examples/","page":"Real examples","title":"Real examples","text":"In tennis, it is sufficient to summarize the skills in a single dimension since the log_evidence is maximized when the parameters of the surface's factors (i.e. sigma and gamma) vanish.  In other examples, where the multi-dimensionality of skills could be more relevant, it should be necessary to model the skills of all agents using different components.","category":"page"},{"location":"man/examples/","page":"Real examples","title":"Real examples","text":"If we consider only the games in which Nadal participates, optimality is achieved when the parameters take the values sigma=035 and gamma=0, meaning that it is necessary to model multidimensional skills (sigma0) but considering that their effect does not change over time (gamma = 0).","category":"page"},{"location":"man/examples/","page":"Real examples","title":"Real examples","text":"nadal_log_evidence = ttt.log_evidence(h_ground, agents = [\"n409\"])","category":"page"},{"location":"man/examples/","page":"Real examples","title":"Real examples","text":"In this scenario, Nadal's ability on Clay is 087beta higher than on Hard and 105beta higher than on Grass. ","category":"page"},{"location":"man/examples/#Plotting-your-own-learning-curves","page":"Real examples","title":"Plotting your own learning curves","text":"","category":"section"},{"location":"man/examples/","page":"Real examples","title":"Real examples","text":"Here is an example to plot the learning curves of the class History. First solve your own example. Here is a dummy example.","category":"page"},{"location":"man/examples/","page":"Real examples","title":"Real examples","text":"composition = [[[string(rand('a':'e'))], [string(rand('a':'e'))]] for i in 1:1000]\nh = ttt.History(composition=composition, gamma=0.03, sigma=1.0)\nttt.convergence(h)","category":"page"},{"location":"man/examples/","page":"Real examples","title":"Real examples","text":"Then, plot the learning curves with an uncertainty band.","category":"page"},{"location":"man/examples/","page":"Real examples","title":"Real examples","text":"using Plots\n\nlc = ttt.learning_curves(h)\nagents = collect(keys(h.agents))[1:3] # select some agents\n\n# Plot all the learning_curves\npp = plot(xlabel=\"t\", ylabel=\"mu\", title=\"Learning Curves\")\nfor (i, agent) in enumerate(agents)\n    t = [v[1] for v in lc[agent] ]\n    mu = [v[2].mu for v in lc[agent] ]\n    sigma = [v[2].sigma for v in lc[agent] ]\n    plot!(t, mu, color=i, label=agent)\n    plot!(t, mu.+sigma, fillrange=mu.-sigma, alpha=0.2,color=i, label=false)\nend\ndisplay(pp)","category":"page"},{"location":"#TrueSkillThroughTime.jl","page":"TrueSkillThroughTime.jl","title":"TrueSkillThroughTime.jl","text":"","category":"section"},{"location":"","page":"TrueSkillThroughTime.jl","title":"TrueSkillThroughTime.jl","text":"The state-of-the-art skill model: Individual learning curves with reliable initial estimates and guaranteed comparability between distant estimates.","category":"page"},{"location":"#Install","page":"TrueSkillThroughTime.jl","title":"Install","text":"","category":"section"},{"location":"","page":"TrueSkillThroughTime.jl","title":"TrueSkillThroughTime.jl","text":"using Pkg\nPkg.add(\"TrueSkillThroughTime\")","category":"page"},{"location":"","page":"TrueSkillThroughTime.jl","title":"TrueSkillThroughTime.jl","text":"using TrueSkillThroughTime\nttt = TrueSkillThroughTime","category":"page"},{"location":"","page":"TrueSkillThroughTime.jl","title":"TrueSkillThroughTime.jl","text":"The we can use it.","category":"page"},{"location":"","page":"TrueSkillThroughTime.jl","title":"TrueSkillThroughTime.jl","text":"using TrueSkillThroughTime\nttt = TrueSkillThroughTime","category":"page"},{"location":"","page":"TrueSkillThroughTime.jl","title":"TrueSkillThroughTime.jl","text":"To appreciate the advantages of TrueSkill Through Time, scroll down to subsection Ilustration\nTo quickly see how to use the package, scroll down to subsection First examples.","category":"page"},{"location":"#Index","page":"TrueSkillThroughTime.jl","title":"Index","text":"","category":"section"},{"location":"","page":"TrueSkillThroughTime.jl","title":"TrueSkillThroughTime.jl","text":"Pages = [\"man/causal.md\", \"man/gaussian.md\", \"man/player.md\", \"man/game.md\", \"man/history.md\", \"man/examples.md\"]\nDepth = 1","category":"page"},{"location":"#ilustration","page":"TrueSkillThroughTime.jl","title":"Ilustration","text":"","category":"section"},{"location":"","page":"TrueSkillThroughTime.jl","title":"TrueSkillThroughTime.jl","text":"To appreciate the advantages of TrueSkill Through Time, let's see how it works in a real case. The following figure presents the estimated learning curves of some famous male players in ATP's history, which we identified using different colors (to see the source code go to section Real examples).","category":"page"},{"location":"","page":"TrueSkillThroughTime.jl","title":"TrueSkillThroughTime.jl","text":"(Image: )","category":"page"},{"location":"","page":"TrueSkillThroughTime.jl","title":"TrueSkillThroughTime.jl","text":"The top bar indicates which player was at the top of the ATP's ranking (the bar has no color when player number 1 is not included among the 10 players identified with colors). There is a relative coincidence between the skill estimates and who is at any given moment at the top of the ATP rankings. However, TrueSkill Through Time allows comparing the relative ability of players over time: the 10th player in the historical ATP's ranking, Hewitt, is a product of the window of opportunity that was opened in the year 2000; and the 4th most skilled player, Murray, is ranked 14th just above Nastase.","category":"page"},{"location":"","page":"TrueSkillThroughTime.jl","title":"TrueSkillThroughTime.jl","text":"The models commonly used in industry and academia (TrueSkill, Glicko, Item-Response Theory) propagates information from past events to future events. Because this approach is an ad-hoc procedure that does not arise from any probabilistic model, its estimates have a number of problems.","category":"page"},{"location":"","page":"TrueSkillThroughTime.jl","title":"TrueSkillThroughTime.jl","text":"(Image: )","category":"page"},{"location":"","page":"TrueSkillThroughTime.jl","title":"TrueSkillThroughTime.jl","text":"The advantage of TrueSkill Through Time lies in its temporal causal model, that links all historical activities in the same Bayesian network, guaranteeing reliable initial estimates and comparability between distant estimates.","category":"page"},{"location":"#first_examples","page":"TrueSkillThroughTime.jl","title":"First examples","text":"","category":"section"},{"location":"","page":"TrueSkillThroughTime.jl","title":"TrueSkillThroughTime.jl","text":"We can update our skill estimates after a single event, or we can estimate the learning curves of all players from a history of events. Let's see both cases.","category":"page"},{"location":"#A-single-game","page":"TrueSkillThroughTime.jl","title":"A single game","text":"","category":"section"},{"location":"","page":"TrueSkillThroughTime.jl","title":"TrueSkillThroughTime.jl","text":"We use the Game class to model events and perform inference. The features of the agents are defined within Player class.","category":"page"},{"location":"","page":"TrueSkillThroughTime.jl","title":"TrueSkillThroughTime.jl","text":"a1 = ttt.Player(); a2 = ttt.Player(); a3 = ttt.Player(); a4 = ttt.Player()\nteam_a = [ a1, a2 ]\nteam_b = [ a3, a4 ]\ng = ttt.Game([team_a, team_b])\nttt.posteriors(g)","category":"page"},{"location":"","page":"TrueSkillThroughTime.jl","title":"TrueSkillThroughTime.jl","text":"where the teams' order in the list implicitly defines the game's result: the teams appearing first in the list (lower index) beat those appearing later (higher index).  This is one of the simplest usage examples. Later on, we will learn how to explicitly specify the result, and others features.","category":"page"},{"location":"#A-history-of-events","page":"TrueSkillThroughTime.jl","title":"A history of events","text":"","category":"section"},{"location":"","page":"TrueSkillThroughTime.jl","title":"TrueSkillThroughTime.jl","text":"We use the History class to compute the learning curves and predictions of a sequence of events. We will define the composition of each game using the names of the agents (i.e. their identifiers). In the following example, all agents (\"a\", \"b\", \"c\") win one game and lose the other.  The results will be implicitly defined by the order in which the game compositions are initialized: the teams appearing firstly in the list defeat those appearing later.  By initializing gamma = 0.0 we specify that skills do not change over time. In this example, where all agents beat each other and their skills do not change over time, the data suggest that all agents have the same skill.","category":"page"},{"location":"","page":"TrueSkillThroughTime.jl","title":"TrueSkillThroughTime.jl","text":"c1 = [[\"a\"],[\"b\"]]\nc2 = [[\"b\"],[\"c\"]]\nc3 = [[\"c\"],[\"a\"]]\ncomposition = [c1, c2, c3]\nh = ttt.History(composition, gamma=0.0)","category":"page"},{"location":"","page":"TrueSkillThroughTime.jl","title":"TrueSkillThroughTime.jl","text":"After initialization, the History class immediately instantiates a new player for each name and activates the computation of the TrueSkill estimates (not yet TrueSkill Through Time). To access them we can call the method learning\\_curves(), which returns a dictionary indexed by the names of the agents.","category":"page"},{"location":"","page":"TrueSkillThroughTime.jl","title":"TrueSkillThroughTime.jl","text":"ttt.learning_curves(h)[\"a\"]","category":"page"},{"location":"","page":"TrueSkillThroughTime.jl","title":"TrueSkillThroughTime.jl","text":"ttt.learning_curves(h)[\"b\"]","category":"page"},{"location":"","page":"TrueSkillThroughTime.jl","title":"TrueSkillThroughTime.jl","text":"Individual learning curves are lists of tuples: each tuple has the time of the estimate as the first component and the estimate itself as the second one. Although in this example no player is stronger than the others, the TrueSkill estimates present strong variations between players. TrueSkill Through Time solves TrueSkill's inability to obtain correct estimates by allowing the information to propagate throughout the system. To compute them, we call the method convergence() of the History class.","category":"page"},{"location":"","page":"TrueSkillThroughTime.jl","title":"TrueSkillThroughTime.jl","text":"ttt.convergence(h)\nttt.learning_curves(h)[\"a\"]","category":"page"},{"location":"","page":"TrueSkillThroughTime.jl","title":"TrueSkillThroughTime.jl","text":"ttt.learning_curves(h)[\"b\"]","category":"page"},{"location":"","page":"TrueSkillThroughTime.jl","title":"TrueSkillThroughTime.jl","text":"TrueSkill Through Time not only returns correct estimates (same for all players), they also have less uncertainty.","category":"page"},{"location":"man/game/#The-Game-class","page":"The Game class","title":"The Game class","text":"","category":"section"},{"location":"man/game/","page":"The Game class","title":"The Game class","text":"using TrueSkillThroughTime\nttt = TrueSkillThroughTime","category":"page"},{"location":"man/game/","page":"The Game class","title":"The Game class","text":"We use the Game class to model events and perform inference.","category":"page"},{"location":"man/game/","page":"The Game class","title":"The Game class","text":"Pages = [\"game.md\"]","category":"page"},{"location":"man/game/","page":"The Game class","title":"The Game class","text":"Game","category":"page"},{"location":"man/game/#TrueSkillThroughTime.Game","page":"The Game class","title":"TrueSkillThroughTime.Game","text":"The Game class\n\nGame(teams::Vector{Vector{Player}}, result::Vector{Float64}, p_draw::Float64, weights::Vector{Vector{Float64}})\n\nProperties:\n\nteams::Vector{Vector{Player}}\nresult::Vector{Float64}\np_draw::Float64\nweights::Vector{Vector{Float64}}\nlikelihoods::Vector{Vector{Gaussian}}\nevidence::Float64\n\n\n\n\n\n","category":"type"},{"location":"man/game/","page":"The Game class","title":"The Game class","text":"Let us return to the example seen on the first page of this manual.","category":"page"},{"location":"man/game/","page":"The Game class","title":"The Game class","text":"a1 = ttt.Player(); a2 = ttt.Player(); a3 = ttt.Player(); a4 = ttt.Player()\nteam_a = [ a1, a2 ]\nteam_b = [ a3, a4 ]\ng = ttt.Game([team_a, team_b])\ng.teams","category":"page"},{"location":"man/game/","page":"The Game class","title":"The Game class","text":"where the teams' order in the list implicitly defines the game's result: the teams appearing first in the list (lower index) beat those appearing later (higher index). ","category":"page"},{"location":"man/game/#Evidence-and-likelihood","page":"The Game class","title":"Evidence and likelihood","text":"","category":"section"},{"location":"man/game/","page":"The Game class","title":"The Game class","text":"During the initialization, the Game class computes the prior prediction of the observed result (the evidence property) and the approximate likelihood of each player (the likelihoods property).","category":"page"},{"location":"man/game/","page":"The Game class","title":"The Game class","text":"lhs = g.likelihoods\nround(g.evidence, digits=3)","category":"page"},{"location":"man/game/","page":"The Game class","title":"The Game class","text":"In this case, the evidence is 05 because both teams had the same prior skill estimates.","category":"page"},{"location":"man/game/#Posterior","page":"The Game class","title":"Posterior","text":"","category":"section"},{"location":"man/game/","page":"The Game class","title":"The Game class","text":"The method posteriors() of class Game to compute the posteriors.","category":"page"},{"location":"man/game/","page":"The Game class","title":"The Game class","text":"posteriors","category":"page"},{"location":"man/game/#TrueSkillThroughTime.posteriors","page":"The Game class","title":"TrueSkillThroughTime.posteriors","text":"posteriors(g::Game)\n\n\n\n\n\n","category":"function"},{"location":"man/game/","page":"The Game class","title":"The Game class","text":"pos = ttt.posteriors(g)\npos[1][1]","category":"page"},{"location":"man/game/","page":"The Game class","title":"The Game class","text":"Posteriors can also be found by manually multiplying the likelihoods and priors. ","category":"page"},{"location":"man/game/","page":"The Game class","title":"The Game class","text":"lhs[1][1] * a1.prior","category":"page"},{"location":"man/game/#Team-performance","page":"The Game class","title":"Team performance","text":"","category":"section"},{"location":"man/game/","page":"The Game class","title":"The Game class","text":"performance(G::Game,i::Int64)","category":"page"},{"location":"man/game/#TrueSkillThroughTime.performance-Tuple{Game, Int64}","page":"The Game class","title":"TrueSkillThroughTime.performance","text":"performance(G::Game,i::Int64)\n\n\n\n\n\n","category":"method"},{"location":"man/game/","page":"The Game class","title":"The Game class","text":"We can obtain the expected performance of the first team. ","category":"page"},{"location":"man/game/","page":"The Game class","title":"The Game class","text":"ttt.performance(g,1)","category":"page"},{"location":"man/game/#Full-example","page":"The Game class","title":"Full example","text":"","category":"section"},{"location":"man/game/","page":"The Game class","title":"The Game class","text":"We now analyze a more complex example in which the same four players participate in a multi-team game. The players are organized into three teams of different sizes: two teams with only one player and the other with two players.  The result has a single winning team and a tie between the other two losing teams. Unlike the previous example, we need to use a draw probability greater than zero.","category":"page"},{"location":"man/game/","page":"The Game class","title":"The Game class","text":"ta = [a1]\ntb = [a2, a3]\ntc = [a4]\nteams_3 = [ta, tb, tc]\nresult = [1., 0., 0.]\ng = ttt.Game(teams_3, result, p_draw=0.25)\ng.result","category":"page"},{"location":"man/game/","page":"The Game class","title":"The Game class","text":"The team with the highest score is the winner, and the teams with the same score are tied. In this way, we can specify any outcome including global draws. The evidence and posteriors can be queried in the same way as before.","category":"page"},{"location":"man/game/","page":"The Game class","title":"The Game class","text":"ttt.posteriors(g)","category":"page"}]
}
