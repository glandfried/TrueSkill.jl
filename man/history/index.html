<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>The History class · TrueSkillThroughTime.jl</title><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.039/juliamono-regular.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.jpeg" alt="TrueSkillThroughTime.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">TrueSkillThroughTime.jl</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">TrueSkillThroughTime.jl</a></li><li><span class="tocitem">Sections</span><ul><li><a class="tocitem" href="../causal/">Causal model</a></li><li><a class="tocitem" href="../gaussian/">The <code>Gaussian</code> class</a></li><li><a class="tocitem" href="../player/">The <code>Player</code> class</a></li><li><a class="tocitem" href="../game/">The <code>Game</code> class</a></li><li class="is-active"><a class="tocitem" href>The <code>History</code> class</a><ul class="internal"><li><a class="tocitem" href="#Learning-curves"><span>Learning curves</span></a></li><li><a class="tocitem" href="#Convergence"><span>Convergence</span></a></li><li><a class="tocitem" href="#Model-evidence"><span>Model evidence</span></a></li><li><a class="tocitem" href="#Optimizing-the-dynamic-factor"><span>Optimizing the dynamic factor</span></a></li></ul></li><li><a class="tocitem" href="../examples/">Real examples</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Sections</a></li><li class="is-active"><a href>The <code>History</code> class</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>The <code>History</code> class</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/glandfried/TrueSkillThroughTime.jl/blob/master/docs/src/man/history.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="The-History-class"><a class="docs-heading-anchor" href="#The-History-class">The <code>History</code> class</a><a id="The-History-class-1"></a><a class="docs-heading-anchor-permalink" href="#The-History-class" title="Permalink"></a></h1><p>We use the <code>History</code> class to compute the learning curves and predictions of a sequence of events.</p><ul><li><a href="#The-History-class">The <code>History</code> class</a></li><ul><li><a href="#Learning-curves">Learning curves</a></li><li><a href="#Convergence">Convergence</a></li><li><a href="#Model-evidence">Model evidence</a></li><li><a href="#Optimizing-the-dynamic-factor">Optimizing the dynamic factor</a></li></ul></ul><article class="docstring"><header><a class="docstring-binding" id="TrueSkillThroughTime.History" href="#TrueSkillThroughTime.History"><code>TrueSkillThroughTime.History</code></a> — <span class="docstring-category">Type</span></header><section><div><p>The <code>History</code> class</p><pre><code class="nohighlight hljs">History(composition::Vector{Vector{Vector{String}}},
results::Vector{Vector{Float64}}=Vector{Vector{Float64}}(),
times::Vector{Int64}=Int64[], priors::Dict{String,Player}=Dict{String,Player}()
; mu::Float64=MU, sigma::Float64=SIGMA, beta::Float64=BETA,
gamma::Float64=GAMMA, p_draw::Float64=P_DRAW, online::Bool=false,
weights::Vector{Vector{Vector{Float64}}}=Vector{Vector{Vector{Float64}}}())</code></pre><p>Properties:</p><pre><code class="nohighlight hljs">size::Int64
batches::Vector{Batch}
agents::Dict{String,Agent}
time::Bool
mu::Float64
sigma::Float64
beta::Float64
gamma::Float64
p_draw::Float64
online::Bool</code></pre></div></section></article><p>Let us return to the example seen on the first page of this manual. We define the composition of each game using the names of the agents (i.e. their identifiers). In the following example, all agents (<code>&quot;a&quot;, &quot;b&quot;, &quot;c&quot;</code>) win one game and lose the other.  The results will be implicitly defined by the order in which the game compositions are initialized: the teams appearing firstly in the list defeat those appearing later.  By initializing <code>gamma = 0.0</code> we specify that skills do not change over time.</p><pre><code class="language-julia hljs">c1 = [[&quot;a&quot;],[&quot;b&quot;]]
c2 = [[&quot;b&quot;],[&quot;c&quot;]]
c3 = [[&quot;c&quot;],[&quot;a&quot;]]
composition = [c1, c2, c3]
h = ttt.History(composition, gamma=0.0)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">History(Events=3, Batches=3, Agents=3)</code></pre><p>After initialization, the <code>History</code> class immediately instantiates a new player for each name and activates the computation of the TrueSkill estimates (not yet TrueSkill Through Time).</p><h2 id="Learning-curves"><a class="docs-heading-anchor" href="#Learning-curves">Learning curves</a><a id="Learning-curves-1"></a><a class="docs-heading-anchor-permalink" href="#Learning-curves" title="Permalink"></a></h2><p>To access estimates we can call the method <code>learning\_curves()</code>, which returns a dictionary indexed by the names of the agents.</p><article class="docstring"><header><a class="docstring-binding" id="TrueSkillThroughTime.learning_curves" href="#TrueSkillThroughTime.learning_curves"><code>TrueSkillThroughTime.learning_curves</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">learning_curves(h::History)</code></pre></div></section></article><pre><code class="language-julia hljs">ttt.learning_curves(h)[&quot;a&quot;]</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">2-element Array{Tuple{Int64,Gaussian},1}:
 (1, Gaussian(mu=3.339079, sigma=4.985033))
 (3, Gaussian(mu=-2.687824, sigma=3.77941))</code></pre><pre><code class="language-julia hljs">ttt.learning_curves(h)[&quot;b&quot;]</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">2-element Array{Tuple{Int64,Gaussian},1}:
 (1, Gaussian(mu=-3.339079, sigma=4.985033))
 (2, Gaussian(mu=0.058622, sigma=4.218053))</code></pre><p>Individual learning curves are lists of tuples: each tuple has the time of the estimate as the first component and the estimate itself as the second one. Although in this example no player is stronger than the others, the TrueSkill estimates present strong variations between players.</p><h2 id="Convergence"><a class="docs-heading-anchor" href="#Convergence">Convergence</a><a id="Convergence-1"></a><a class="docs-heading-anchor-permalink" href="#Convergence" title="Permalink"></a></h2><p>TrueSkill Through Time solves TrueSkill&#39;s inability to obtain correct estimates by allowing the information to propagate throughout the system. To compute them, we call the method <code>convergence()</code> of the <code>History</code> class.</p><article class="docstring"><header><a class="docstring-binding" id="TrueSkillThroughTime.convergence-Tuple{History}" href="#TrueSkillThroughTime.convergence-Tuple{History}"><code>TrueSkillThroughTime.convergence</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">convergence(h::History; epsilon::Float64=EPSILON,
iterations::Int64=ITERATIONS; epsilon::Float64=EPSILON, iterations::Int64=ITERATIONS, verbose = true)</code></pre></div></section></article><p>TrueSkill Through Time not only returns correct estimates (same for all players), they also have less uncertainty.</p><pre><code class="language-julia hljs">ttt.convergence(h)
ttt.learning_curves(h)[&quot;a&quot;]</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">2-element Array{Tuple{Int64,Gaussian},1}:
 (1, Gaussian(mu=0.0, sigma=2.394808))
 (3, Gaussian(mu=-0.0, sigma=2.394808))</code></pre><pre><code class="language-julia hljs">ttt.learning_curves(h)[&quot;b&quot;]</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">2-element Array{Tuple{Int64,Gaussian},1}:
 (1, Gaussian(mu=-0.0, sigma=2.394808))
 (2, Gaussian(mu=-0.0, sigma=2.394808))</code></pre><h2 id="Model-evidence"><a class="docs-heading-anchor" href="#Model-evidence">Model evidence</a><a id="Model-evidence-1"></a><a class="docs-heading-anchor-permalink" href="#Model-evidence" title="Permalink"></a></h2><p>We would like to have a procedure to decide whether TrueSkill Through Time is better than others models and the optimal values of the parameters <span>$\sigma$</span> and <span>$\gamma$</span>. In the same way that we use probability theory to evaluate the hypotheses of a model given the data, we can also evaluate different models given the data.</p><p><span>$P(\text{Model}|\text{Data}) \propto P(\text{Data}|\text{Model})P(\text{Model})$</span></p><p>where <span>$P(\text{Model})$</span> is the prior of the models, which we define, and <span>$P(\text{Data}|\text{Model})$</span> is the prediction made by the model. In the special case where we have no prior preference over any model, we need only compare the predictions made by the models.</p><p><span>$P(\text{Model}|\text{Data}) \propto P(\text{Data}|\text{Model})$</span></p><p>In other words, we prefer the model with the best prediction.</p><p><span>$P(\text{Data}|\text{Model}) = P(d_1|\text{M})P(d_2|d_1,\text{M}) \dots P(d_n|d_{n-1}, \dots, d_1, \text{M})$</span></p><p>where D represents the data set, M the model, and <span>$d_i$</span> the individual data points. This measure can be obtained by the <code>evidence</code> method.</p><article class="docstring"><header><a class="docstring-binding" id="TrueSkillThroughTime.log_evidence" href="#TrueSkillThroughTime.log_evidence"><code>TrueSkillThroughTime.log_evidence</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">log_evidence(h::History; agents::Vector{String} = Vector{String}(), forward::Bool = false)</code></pre></div></section></article><p>Let us develop a complex synthetic example in which this measure is useful for choosing the optimal dynamic uncertainty.</p><h2 id="Optimizing-the-dynamic-factor"><a class="docs-heading-anchor" href="#Optimizing-the-dynamic-factor">Optimizing the dynamic factor</a><a id="Optimizing-the-dynamic-factor-1"></a><a class="docs-heading-anchor-permalink" href="#Optimizing-the-dynamic-factor" title="Permalink"></a></h2><p>We now analyze a scenario in which a new player joins a large community of already known players. In this example, we focus on the estimation of an evolving skill. For this purpose, we establish the skill of the target player to change over time following a logistic function. The community is generated by ensuring that each opponent has a skill similar to that of the target player throughout their evolution.  In the following code, we generate the target player&#39;s learning curve and 1000 random opponents. </p><pre><code class="language-julia hljs">using Random; Random.seed!(999); N = 1000
function skill(experience, middle, maximum, slope)
    return maximum/(1+exp(slope*(-experience+middle)))
end
target = skill.(1:N, 500, 2, 0.0075)
opponents = Random.randn.(1000)*0.5 .+ target
println(&quot;t1 = &quot;, target[1], &quot;, tn = &quot;, target[end])</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">t1 = 0.046292688460268065, tn = 1.9540452601799487</code></pre><p>The list <code>target</code> has the agent&#39;s skills at each moment: the values start at zero and grow smoothly until the target player&#39;s skill reaches two. The list <code>opponents</code> includes the randomly generated opponents&#39; skills following a Gaussian distribution centered on each target&#39;s skills and a standard deviation of <span>$0.5$</span>.</p><pre><code class="language-julia hljs">composition = [[[&quot;a&quot;], [string(i)]] for i in 1:N]
results = [r ? [1.,0.] : [0.,1.] for r in (Random.randn(N).+target.&gt;Random.randn(N).+opponents)]
times = [i for i in 1:N]
priors = Dict{String,ttt.Player}()
for i in 1:N  priors[string(i)] = ttt.Player(ttt.Gaussian(opponents[i], 0.2))  end

h = ttt.History(composition, results, times, priors, gamma=0.018)
ttt.convergence(h, iterations = 16)
mu = [tp[2].mu for tp in ttt.learning_curves(h)[&quot;a&quot;]]
println()</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Iteration = 1, step = (4.939046229377598, 3.629809564846882)
Iteration = 2, step = (1.1540578892410558e-5, 1.5807710139675857e-5)
Iteration = 3, step = (5.0096926607068326e-11, 6.896255788646499e-11)
End</code></pre><p>In this code, we define four variables to instantiate the class <code>History</code> to compute the target&#39;s learning curve. The variable <code>composition</code> contains 1000 games between the target player and different opponents. The list <code>results</code> is generated randomly by sampling the agents&#39; performance following Gaussian distributions centered on their skills. The winner is the player with the highest performance. The variable <code>time</code> is a list of integer values ranging from 0 to 999 representing the time batch in which each game is located: the class <code>History</code> uses the temporal distance between events to determine the amount of dynamic uncertainty (<span>$\gamma^2$</span>) to be added between games. The variable <code>priors</code> is a dictionary used to customize player attributes: we assign low uncertainty to the opponents&#39; priors as we know their skills beforehand.</p><p>The class <code>History</code> receives these four parameters and initializes the target player using the default values and a dynamic uncertainty <code>gamma=0.018</code>. Using the method <code>convergence()</code>, we obtain the TrueSkill Through Time estimates and the target&#39;s learning curve. The following figure shows the evolution of the true (solid line) and estimated (dotted line) target player&#39;s learning curves.</p><p><img src="../../../../static/logistic0.png" alt/></p><p>The estimated learning curves remain close to the actual skill during the whole evolution.</p><pre><code class="language-julia hljs">le = ttt.log_evidence(h)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">-664.5495316429129</code></pre><p>The geometric mean of the evidence is</p><pre><code class="language-julia hljs">exp(le/h.size)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">0.5145052438442269</code></pre><p>To optimize, repeat this procedure with different values of gamma until minimize the <code>log_evidence</code> (or maximize the geommetric mean). </p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../game/">« The <code>Game</code> class</a><a class="docs-footer-nextpage" href="../examples/">Real examples »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.7 on <span class="colophon-date" title="Monday 1 November 2021 08:33">Monday 1 November 2021</span>. Using Julia version 1.5.0.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
